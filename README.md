# ğŸ¤Ÿ Sign Language to Voice and Text Translator

This mobile application translates **sign language gestures** into **text** and **voice output** using advanced AI and computer vision techniques. The solution aims to bridge the communication gap for the deaf and hard-of-hearing community by offering real-time interpretation using a mobile device.

---

## ğŸ“Œ Project Overview

The project leverages cutting-edge deep learning models and computer vision to:

- Capture real-time hand gestures via a camera
- Recognize and interpret signs using a GRU-based model
- Translate the signs into text
- Convert the recognized text to human-like voice using TTS

Developed as a part of our Artificial Intelligence coursework to apply theoretical knowledge in a real-world assistive technology solution.

---

## ğŸ‘¥ Team Members

- **Pema Chozom**
- **Pema Yangchen**
- **Thukten Dema**
- **Asseh Nepal**
- **Bidash Gurung**

---

## ğŸ§  Technologies & Tools Used

| Technology        | Purpose                                       |
|-------------------|-----------------------------------------------|
| **Flutter**        | Cross-platform mobile app development         |
| **OpenCV**         | Real-time gesture recognition (Computer Vision) |
| **GRU Model**      | Sequential learning for sign recognition      |
| **Text-to-Speech** | Voice output from translated text             |
| **Python**         | Model training and preprocessing              |
| **TensorFlow Lite**| AI model integration on mobile devices        |

---

## ğŸ“± App Features

- ğŸ“· Real-time hand gesture recognition using phone camera
- ğŸ“ Live text output of recognized sign language
- ğŸ”Š Text-to-Speech (TTS) conversion from recognized text
- ğŸŒ Offline support with embedded TensorFlow Lite models
- ğŸ§© Simple, intuitive, and accessible user interface

---

## ğŸ¨ UI/UX Design

The UI is designed with accessibility in mind:

- âœ… Large buttons for easy interaction  
- âœ… Live camera feedback for gesture tracking  
- âœ… Toggle switch to enable/disable voice output  
- âœ… Simple step-by-step instructions for usage  

---

## ğŸ¯ Learning Outcomes

- ğŸ¤– Gained hands-on experience training and deploying GRU models
- ğŸ¥ Learned computer vision basics for gesture recognition using OpenCV
- ğŸ“² Understood how to embed ML models in Flutter using TensorFlow Lite
- ğŸ”Š Implemented Text-to-Speech features for accessibility
- ğŸ”— Successfully integrated AI models into real-time mobile apps
- ğŸ¤ Strengthened teamwork and project collaboration in a tech team
- ğŸ§  Enhanced problem-solving and debugging skills in cross-platform apps

---

## âœ… Project Status

- âœ”ï¸ MVP completed and tested
- âœ”ï¸ Fully functional offline gesture-to-text-to-speech translation
- âœ”ï¸ Presented as a part of coursework in Artificial Intelligence

---

## ğŸš€ Future Improvements

- ğŸ”„ Extend gesture recognition to full sentence structures
- ğŸŒ Add multi-language TTS support
- ğŸ‘¤ Introduce user training for personalized gesture sets
- ğŸ¤– Use transformer models for improved contextual understanding
- ğŸ‡§ğŸ‡¹ Expand dataset to support Bhutanese Sign Language (BSL)

